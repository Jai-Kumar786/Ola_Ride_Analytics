{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Day 15: ML Feature Engineering & Data Prep\n",
    "\n",
    "We've successfully built a comprehensive analytics application, and now it's time to add the final, most advanced component. Welcome to **Day 15: ML Feature Engineering & Data Prep**.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Today marks our transition from data analytics (describing the past) to machine learning (predicting the future). Our goal is to take the clean dataset we've been using and meticulously transform it into a format that a machine learning model can understand. This process, known as **feature engineering**, is the absolute foundation of a successful predictive model.\n",
    "\n",
    "We will create a new Jupyter Notebook for this entire machine learning workflow to keep it organized and separate from our earlier exploratory analysis.\n",
    "\n",
    "## Task 2: Save and Run the Notebook\n",
    "\n",
    "### Action Steps\n",
    "\n",
    "1. Create a new Jupyter Notebook file in your `notebooks` folder\n",
    "2. Name it `05_ml_feature_engineering.ipynb`\n",
    "3. Copy the code from the first file into this new notebook\n",
    "4. Run all the cells from top to bottom\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "The script will print out status messages at each step of the process. The final output will be the first five rows of your fully processed training data, `X_train_scaled`. It should look like a table of numbers, where:\n",
    "\n",
    "- All categorical text has been converted\n",
    "- All values have been scaled\n",
    "\n",
    "## Completion\n",
    "\n",
    "**Congratulations** on completing Day 15! You have successfully navigated the most complex and critical part of the machine learning lifecycle: data preparation and feature engineering.\n",
    "\n",
    "You now have clean, robust training and testing datasets that are perfectly formatted for our next step. Tomorrow, on **Day 16**, we will use this prepared data to train our very first predictive models."
   ],
   "id": "df2c3c353d3faa43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T08:59:05.846560Z",
     "start_time": "2025-09-23T08:59:03.752701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"--- Starting ML Feature Engineering Process ---\")\n",
    "\n",
    "# --- 1. Load the Enhanced Dataset ---\n",
    "try:\n",
    "    df = pd.read_csv('../data/ola_data_enhanced.csv')\n",
    "    df['booking_timestamp'] = pd.to_datetime(df['booking_timestamp'])\n",
    "    print(\"✅ Successfully loaded the enhanced dataset.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: 'ola_data_enhanced.csv' not found. Please ensure the file exists.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Define the Prediction Target (y) ---\n",
    "# Our goal is to predict if a ride will be Canceled by the Customer.\n",
    "df['is_cancelled'] = (df['booking_status'] == 'Canceled by Customer').astype(int)\n",
    "print(f\"\\nTarget variable 'is_cancelled' created.\")\n",
    "print(df['is_cancelled'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# --- 3. Select Predictive Features (X) ---\n",
    "# We select columns that we hypothesize will influence a customer's decision to cancel.\n",
    "# We exclude identifiers, timestamps, and post-booking info like ratings.\n",
    "features = [\n",
    "    'v_tat',\n",
    "    'c_tat',\n",
    "    'booking_value',\n",
    "    'ride_distance',\n",
    "    'hour_of_day',\n",
    "    'vehicle_type',\n",
    "    'payment_method'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['is_cancelled']\n",
    "\n",
    "print(f\"\\nSelected {len(features)} features for the model.\")\n",
    "\n",
    "\n",
    "# --- 4. Preprocessing Step 1: Handle Missing Values ---\n",
    "# ML models cannot handle missing (NaN) values. We'll use a simple median imputation strategy.\n",
    "for col in ['v_tat', 'c_tat']:\n",
    "    median_val = X[col].median()\n",
    "    X[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# For categorical columns, we'll fill with the most frequent value (mode).\n",
    "X['payment_method'].fillna(X['payment_method'].mode()[0], inplace=True)\n",
    "print(\"\\n✅ Handled missing values using median and mode imputation.\")\n",
    "\n",
    "\n",
    "# --- 5. Preprocessing Step 2: One-Hot Encode Categorical Features ---\n",
    "# Models only understand numbers, so we convert text categories into numeric format.\n",
    "categorical_cols = ['vehicle_type', 'payment_method']\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "print(\"✅ Converted categorical features to numeric using one-hot encoding.\")\n",
    "print(\"New shape of X:\", X.shape)\n",
    "\n",
    "\n",
    "# --- 6. Preprocessing Step 3: Train-Test Split ---\n",
    "# We split the data BEFORE scaling to prevent data leakage from the test set.\n",
    "# This is a critical step for an unbiased evaluation of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, # 20% of the data will be used for testing\n",
    "    random_state=42, # Ensures the split is the same every time we run the script\n",
    "    stratify=y # Ensures the proportion of cancellations is the same in both train and test sets\n",
    ")\n",
    "print(\"\\n✅ Split data into training (80%) and testing (20%) sets.\")\n",
    "\n",
    "\n",
    "# --- 7. Preprocessing Step 4: Feature Scaling ---\n",
    "# We scale the numeric features so that variables with large values (like booking_value)\n",
    "# don't unfairly dominate variables with small values (like hour_of_day).\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames for easier inspection\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "print(\"✅ Applied standard scaling to numeric features.\")\n",
    "\n",
    "print(\"\\n--- Feature Engineering Complete ---\")\n",
    "print(\"Data is now ready for model training.\")\n",
    "\n",
    "# Display the first few rows of the final prepared training data\n",
    "print(\"\\n--- Head of Processed Training Data (X_train_scaled) ---\")\n",
    "print(X_train_scaled.head())\n",
    "\n",
    "# ... (all the previous code from Day 15) ...\n",
    "\n",
    "# --- 8. Save the Processed Data for Model Training ---\n",
    "# We save the processed datasets to be used in the next notebook.\n",
    "output_dir = '../data/processed/'\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "X_train_scaled.to_csv(os.path.join(output_dir, 'X_train_scaled.csv'), index=False)\n",
    "X_test_scaled.to_csv(os.path.join(output_dir, 'X_test_scaled.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "print(\"\\n✅ All processed data files have been saved to the 'data/processed/' directory.\")\n",
    "\n",
    "# NEW: Save the scaler object\n",
    "scaler_path = '../models/'\n",
    "if not os.path.exists(scaler_path):\n",
    "    os.makedirs(scaler_path)\n",
    "joblib.dump(scaler, os.path.join(scaler_path, 'scaler.joblib'))\n",
    "\n",
    "\n",
    "print(\"\\n✅ All processed data files and the scaler have been saved.\")"
   ],
   "id": "c7fc2ed72d92f10e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ML Feature Engineering Process ---\n",
      "✅ Successfully loaded the enhanced dataset.\n",
      "\n",
      "Target variable 'is_cancelled' created.\n",
      "is_cancelled\n",
      "0    0.898092\n",
      "1    0.101908\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Selected 7 features for the model.\n",
      "\n",
      "✅ Handled missing values using median and mode imputation.\n",
      "✅ Converted categorical features to numeric using one-hot encoding.\n",
      "New shape of X: (103024, 14)\n",
      "\n",
      "✅ Split data into training (80%) and testing (20%) sets.\n",
      "✅ Applied standard scaling to numeric features.\n",
      "\n",
      "--- Feature Engineering Complete ---\n",
      "Data is now ready for model training.\n",
      "\n",
      "--- Head of Processed Training Data (X_train_scaled) ---\n",
      "      v_tat     c_tat  booking_value  ride_distance  hour_of_day  \\\n",
      "0  0.301422  0.003208      -0.516226      -0.011883    -0.507131   \n",
      "1 -0.028459  0.003208      -0.151763      -0.898328     1.084996   \n",
      "2 -0.248380  1.590529       0.928609       0.747926    -1.665042   \n",
      "3 -1.677864 -0.878637      -0.640813      -0.391788     0.795518   \n",
      "4  0.631303 -1.760482      -0.521805       1.634371    -0.651870   \n",
      "\n",
      "   vehicle_type_Bike  vehicle_type_Mini  vehicle_type_Prime Plus  \\\n",
      "0          -0.407537          -0.404191                -0.409539   \n",
      "1          -0.407537          -0.404191                -0.409539   \n",
      "2          -0.407537          -0.404191                -0.409539   \n",
      "3          -0.407537          -0.404191                -0.409539   \n",
      "4          -0.407537          -0.404191                -0.409539   \n",
      "\n",
      "   vehicle_type_Prime SUV  vehicle_type_Prime Sedan  vehicle_type_eBike  \\\n",
      "0               -0.407031                 -0.409963           -0.411597   \n",
      "1                2.456815                 -0.409963           -0.411597   \n",
      "2                2.456815                 -0.409963           -0.411597   \n",
      "3               -0.407031                 -0.409963           -0.411597   \n",
      "4                2.456815                 -0.409963           -0.411597   \n",
      "\n",
      "   payment_method_Credit Card  payment_method_Debit Card  payment_method_UPI  \n",
      "0                   -0.155301                  -0.079373            1.732093  \n",
      "1                   -0.155301                  -0.079373           -0.577336  \n",
      "2                   -0.155301                  -0.079373           -0.577336  \n",
      "3                    6.439110                  -0.079373           -0.577336  \n",
      "4                   -0.155301                  -0.079373           -0.577336  \n",
      "\n",
      "✅ All processed data files have been saved to the 'data/processed/' directory.\n",
      "\n",
      "✅ All processed data files and the scaler have been saved.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Detailed Analysis of Feature Engineering Output\n",
    "\n",
    "Of course. You've successfully run the feature engineering script, and the output you're seeing is the result of transforming our clean, human-readable data into a highly processed, numeric format that a machine learning model can understand. This is a critical and complex process, so let's break down exactly what we did and what each part of the output means.\n",
    "\n",
    "**This is the story of how we prepared our data for a predictive model**.[1]\n",
    "\n",
    "## Target Variable Creation & Class Imbalance\n",
    "\n",
    "```\n",
    "Target variable 'is_cancelled' created.\n",
    "is_cancelled\n",
    "0    0.898092  (Not Cancelled)\n",
    "1    0.101908  (Cancelled by Customer)\n",
    "Name: proportion, dtype: float64\n",
    "```\n",
    "\n",
    "### What We Did\n",
    "We created our **\"target\"** variable—the thing we want to predict. The `is_cancelled` column now contains a 1 if the ride was cancelled by the customer and a 0 otherwise.[1]\n",
    "\n",
    "### What This Means\n",
    "The output shows that only about 10.2% of the bookings in our dataset were cancelled by customers. This is a classic **imbalanced dataset**. This is a very important finding because it tells us that a lazy model could achieve ~90% accuracy by simply guessing \"not cancelled\" every time. That's why we will need to use more advanced metrics (like the F1-score) to evaluate our model's true performance tomorrow.[5]\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "```\n",
    "Selected 7 features for the model.\n",
    "```\n",
    "\n",
    "### What We Did\n",
    "We chose 7 columns from our original dataset that we believe have the power to predict a cancellation before it happens.\n",
    "\n",
    "### What This Means\n",
    "We specifically excluded columns like `booking_id` (just an identifier), `customer_rating` (this happens after the ride, so we can't use it to predict something beforehand), and `cancellation_reason` (this is a result of the cancellation, not a predictor of it).\n",
    "\n",
    "## Handling Missing Values\n",
    "\n",
    "```\n",
    "✅ Handled missing values using median and mode imputation.\n",
    "```\n",
    "\n",
    "### What We Did\n",
    "Machine learning algorithms cannot work with empty (NaN) cells. We filled the missing numerical values (`v_tat`, `c_tat`) with their respective median values and the categorical `payment_method` with its most common value.[2]\n",
    "\n",
    "### What This Means\n",
    "Our dataset is now complete, with no missing information, making it ready for the next steps.\n",
    "\n",
    "## One-Hot Encoding\n",
    "\n",
    "```\n",
    "✅ Converted categorical features to numeric using one-hot encoding.\n",
    "New shape of X: (103024, 14)\n",
    "```\n",
    "\n",
    "### What We Did\n",
    "This is one of the most significant transformations. Models don't understand text like \"Prime Sedan\" or \"Cash\". **One-hot encoding** converts these text columns into new numeric columns.[3][1][5]\n",
    "\n",
    "### What This Means\n",
    "Our feature set X expanded from 7 columns to 14. For example, the single `vehicle_type` column was replaced by several new columns like `vehicle_type_Bike`, `vehicle_type_Mini`, etc. If a ride was a \"Bike\", it will have a 1 in the `vehicle_type_Bike` column and a 0 in all the others.[1][3]\n",
    "\n",
    "## Train-Test Split & Scaling\n",
    "\n",
    "```\n",
    "✅ Split data into training (80%) and testing (20%) sets.\n",
    "✅ Applied standard scaling to numeric features.\n",
    "```\n",
    "\n",
    "### What We Did\n",
    "We divided our data into a training set (which the model learns from) and a test set (which we use for an unbiased evaluation). We then applied `StandardScaler` to put all our numeric features on the same scale.\n",
    "\n",
    "### What This Means\n",
    "This ensures that a feature with large numbers (like `booking_value`) doesn't have an unfair influence on the model over a feature with small numbers (like `hour_of_day`).[5]\n",
    "\n",
    "## The Final Processed Data\n",
    "\n",
    "```\n",
    "--- Head of Processed Training Data (X_train_scaled) ---\n",
    "      v_tat     c_tat  booking_value  ...  payment_method_UPI\n",
    "0  0.301422  0.003208      -0.516226  ...            1.732093\n",
    "1 -0.028459  0.003208      -0.151763  ...           -0.577336\n",
    "2 -0.248380  1.590529       0.928609  ...           -0.577336\n",
    "```\n",
    "\n",
    "### What This Is\n",
    "This table is the final product of today's work. It is a completely numeric, scaled, and processed version of your data, ready to be fed into a machine learning algorithm.\n",
    "\n",
    "### What It Means\n",
    "\n",
    "- **No Text**: All categorical information is now represented by numeric columns (e.g., `payment_method_UPI`)[1]\n",
    "- **Scaled Values**: Notice that all the numbers are now small and centered around zero. This is the result of the `StandardScaler`. A positive number means the original value was above the average for that feature, and a negative number means it was below average\n",
    "- **Ready for Learning**: This numeric representation allows the machine learning models to find mathematical patterns between the features and the `is_cancelled` target we created\n",
    "\n"
   ],
   "id": "b08b8d965b9e0faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "702e637fdb189a33"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
